{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun 17 10:04:49 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 6000 Ada Gene...    Off |   00000000:21:00.0 Off |                  Off |\n",
      "| 30%   38C    P8             25W /  300W |      15MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX 6000 Ada Gene...    Off |   00000000:22:00.0 Off |                  Off |\n",
      "| 30%   50C    P8             29W /  300W |      15MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA RTX 6000 Ada Gene...    Off |   00000000:41:00.0 Off |                  Off |\n",
      "| 30%   44C    P8             28W /  300W |      15MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA RTX 6000 Ada Gene...    Off |   00000000:42:00.0 Off |                  Off |\n",
      "| 30%   49C    P8             27W /  300W |     589MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   1207479      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    1   N/A  N/A   1207479      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    2   N/A  N/A   1207479      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    3   N/A  N/A   1207479      G   /usr/lib/xorg/Xorg                            236MiB |\n",
      "|    3   N/A  N/A   1207836      G   /usr/bin/gnome-shell                          101MiB |\n",
      "|    3   N/A  N/A   1209920      G   ...irefox/4336/usr/lib/firefox/firefox        170MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [7., 8., 9.],\n",
       "        [1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [7., 8., 9.],\n",
       "        [1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [7., 8., 9.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[[1,2,3],[4,5,6],[7,8,9]],[[1,2,3],[4,5,6],[7,8,9]],[[1,2,3],[4,5,6],[7,8,9]]],dtype=torch.float)\n",
    "x.view(9,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(100.9869)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.arange(27,dtype=torch.float).view(3,3,3)\n",
    "y\n",
    "\n",
    "y.view(27)\n",
    "\n",
    "loss = F.cross_entropy(x,y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1301495/1705839279.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  F.softmax(a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3],dtype=torch.float)\n",
    "F.softmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.arange(9).reshape(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2],\n",
       "         [3, 4, 5],\n",
       "         [6, 7, 8]]),\n",
       " tensor([ 9, 12, 15]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = torch.sum(t,dim=0)\n",
    "\n",
    "t,s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2],\n",
       "         [3, 4, 5],\n",
       "         [6, 7, 8]]),\n",
       " tensor([ 3, 12, 21]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = torch.sum(t,dim=1)\n",
    "\n",
    "t,s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2],\n",
       "         [3, 4, 5],\n",
       "         [6, 7, 8]]),\n",
       " tensor([ 3, 12, 21]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = torch.sum(t,dim=-1)\n",
    "\n",
    "t,s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0,  1,  2],\n",
       "          [ 3,  4,  5],\n",
       "          [ 6,  7,  8]],\n",
       " \n",
       "         [[ 9, 10, 11],\n",
       "          [12, 13, 14],\n",
       "          [15, 16, 17]],\n",
       " \n",
       "         [[18, 19, 20],\n",
       "          [21, 22, 23],\n",
       "          [24, 25, 26]]]),\n",
       " tensor([[27, 30, 33],\n",
       "         [36, 39, 42],\n",
       "         [45, 48, 51]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(27).reshape(3,3,3)\n",
    "s = torch.sum(t,dim=0)\n",
    "t,s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0,  1,  2],\n",
       "          [ 3,  4,  5],\n",
       "          [ 6,  7,  8]],\n",
       " \n",
       "         [[ 9, 10, 11],\n",
       "          [12, 13, 14],\n",
       "          [15, 16, 17]],\n",
       " \n",
       "         [[18, 19, 20],\n",
       "          [21, 22, 23],\n",
       "          [24, 25, 26]]]),\n",
       " tensor([[ 9, 12, 15],\n",
       "         [36, 39, 42],\n",
       "         [63, 66, 69]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = torch.sum(t,dim=1)\n",
    "t,s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0,  1,  2],\n",
       "          [ 3,  4,  5],\n",
       "          [ 6,  7,  8]],\n",
       " \n",
       "         [[ 9, 10, 11],\n",
       "          [12, 13, 14],\n",
       "          [15, 16, 17]],\n",
       " \n",
       "         [[18, 19, 20],\n",
       "          [21, 22, 23],\n",
       "          [24, 25, 26]]]),\n",
       " tensor([[ 3, 12, 21],\n",
       "         [30, 39, 48],\n",
       "         [57, 66, 75]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = torch.sum(t,dim=2)\n",
    "t,s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.arange(4*8*65).reshape(4,8,65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 65])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[:,-1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  65,   66,   67,   68,   69,   70,   71,   72,   73,   74,   75,   76,\n",
       "           77,   78,   79,   80,   81,   82,   83,   84,   85,   86,   87,   88,\n",
       "           89,   90,   91,   92,   93,   94,   95,   96,   97,   98,   99,  100,\n",
       "          101,  102,  103,  104,  105,  106,  107,  108,  109,  110,  111,  112,\n",
       "          113,  114,  115,  116,  117,  118,  119,  120,  121,  122,  123,  124,\n",
       "          125,  126,  127,  128,  129],\n",
       "        [ 585,  586,  587,  588,  589,  590,  591,  592,  593,  594,  595,  596,\n",
       "          597,  598,  599,  600,  601,  602,  603,  604,  605,  606,  607,  608,\n",
       "          609,  610,  611,  612,  613,  614,  615,  616,  617,  618,  619,  620,\n",
       "          621,  622,  623,  624,  625,  626,  627,  628,  629,  630,  631,  632,\n",
       "          633,  634,  635,  636,  637,  638,  639,  640,  641,  642,  643,  644,\n",
       "          645,  646,  647,  648,  649],\n",
       "        [1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116,\n",
       "         1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128,\n",
       "         1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140,\n",
       "         1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152,\n",
       "         1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164,\n",
       "         1165, 1166, 1167, 1168, 1169],\n",
       "        [1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636,\n",
       "         1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648,\n",
       "         1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660,\n",
       "         1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672,\n",
       "         1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684,\n",
       "         1685, 1686, 1687, 1688, 1689]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[:,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[[1,2,3],[4,5,6],[7,8,9]],[[11,12,13],[14,15,16],[17,18,19]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3],\n",
       "         [ 4,  5,  6],\n",
       "         [ 7,  8,  9]],\n",
       "\n",
       "        [[11, 12, 13],\n",
       "         [14, 15, 16],\n",
       "         [17, 18, 19]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7,  8,  9],\n",
       "        [17, 18, 19]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode batch, time and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = nn.Embedding(19,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3],\n",
       "         [ 4,  5,  6],\n",
       "         [ 7,  8,  9]],\n",
       "\n",
       "        [[11, 12, 13],\n",
       "         [14, 15, 16],\n",
       "         [17, 18, 19]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix = torch.tensor([1,2,3])\n",
    "ix_emb = emb(ix)\n",
    "ix_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you an see, the embedding actually you loose the real data, you will only get a vector of size embedding dimension added at each value point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ixx = torch.tensor([[1,2,3],[4,5,6]])\n",
    "ixx_emb = emb(ixx)\n",
    "ixx_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings will fail if there is no data corrosponding to the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2205,  0.2218, -1.2863,  0.0289]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([18])\n",
    "emb(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially emedding adds a dimension and removes the actual data from the tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
